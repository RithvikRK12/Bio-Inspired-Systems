{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd70f80d-984e-4998-bf82-6106269251b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 01/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.1s\n",
      "iter 02/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.2s\n",
      "iter 03/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.3s\n",
      "iter 04/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.4s\n",
      "iter 05/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.5s\n",
      "iter 06/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.6s\n",
      "iter 07/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.7s\n",
      "iter 08/15  best=0.980100  (logC=+2.392, logG=-1.905)  elapsed=0.7s\n",
      "iter 09/15  best=0.985075  (logC=+1.938, logG=-2.009)  elapsed=0.8s\n",
      "iter 10/15  best=0.985075  (logC=+1.938, logG=-2.009)  elapsed=0.9s\n",
      "iter 11/15  best=0.985075  (logC=+1.938, logG=-2.009)  elapsed=1.0s\n",
      "iter 12/15  best=0.985075  (logC=+1.938, logG=-2.009)  elapsed=1.1s\n",
      "iter 13/15  best=0.985075  (logC=+1.938, logG=-2.009)  elapsed=1.2s\n",
      "iter 14/15  best=0.985075  (logC=+1.938, logG=-2.009)  elapsed=1.3s\n",
      "iter 15/15  best=0.985075  (logC=+1.938, logG=-2.009)  elapsed=1.4s\n",
      "\n",
      "Optimal Parameter found\n",
      "C = 86.687304 , gamma = 0.009796\n",
      "Best score = 0.985075\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "USE_SKLEARN = True\n",
    "X = None\n",
    "y = None\n",
    "try:\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "except Exception as e:\n",
    "    USE_SKLEARN = False\n",
    "    print(\"sklearn not available, using synthetic fitness fallback:\", e)\n",
    "\n",
    "if USE_SKLEARN:\n",
    "    try:\n",
    "        X, y = make_classification(n_samples=200, n_features=8, n_informative=6,\n",
    "                                   n_redundant=0, n_repeated=0, n_classes=2,\n",
    "                                   class_sep=1.5, flip_y=0.0, random_state=1)\n",
    "    except Exception as e:\n",
    "        USE_SKLEARN = False\n",
    "        X = None; y = None\n",
    "        print(\"failed to create dataset, falling back to synthetic fitness:\", e)\n",
    "\n",
    "def fitness_from_params(logC, logG):\n",
    "    if USE_SKLEARN and X is not None and y is not None:\n",
    "        try:\n",
    "            C = 10 ** logC\n",
    "            gamma = 10 ** logG\n",
    "            clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=0)\n",
    "            scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy', n_jobs=1)\n",
    "            return float(scores.mean())\n",
    "        except Exception as e:\n",
    "            # if something goes wrong with sklearn during runtime, fall back\n",
    "            print(\"sklearn runtime error, switching to fallback fitness:\", e)\n",
    "    # fallback synthetic multimodal fitness\n",
    "    x, yy = float(logC), float(logG)\n",
    "    val = np.exp(-((x - 1.5)**2 + (yy + 1.0)**2))\n",
    "    val *= (0.6 + 0.4 * (np.cos(3*x) * np.cos(3*yy) + 1) / 2)\n",
    "    return float(val)\n",
    "\n",
    "def gwo(n_agents=16, max_iter=20, lb=(-2.0, -4.0), ub=(4.0, -0.5), seed=0, verbose=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dim = 2\n",
    "    lb = np.array(lb); ub = np.array(ub)\n",
    "    positions = np.random.uniform(lb, ub, (n_agents, dim))\n",
    "    fitness = np.array([fitness_from_params(p[0], p[1]) for p in positions])\n",
    "    idx = fitness.argsort()[::-1]\n",
    "    alpha_pos = positions[idx[0]].copy(); alpha_score = float(fitness[idx[0]])\n",
    "    beta_pos  = positions[idx[1]].copy(); beta_score  = float(fitness[idx[1]])\n",
    "    delta_pos = positions[idx[2]].copy(); delta_score = float(fitness[idx[2]])\n",
    "    best_history = [alpha_score]\n",
    "\n",
    "    start = time.time()\n",
    "    for t in range(max_iter):\n",
    "        a = 2.0 * (1.0 - t / float(max_iter))\n",
    "        for i in range(n_agents):\n",
    "            X_i = positions[i].copy()\n",
    "            A1 = 2 * a * np.random.rand() - a\n",
    "            C1 = 2 * np.random.rand()\n",
    "            D_alpha = np.abs(C1 * alpha_pos - X_i)\n",
    "            X1 = alpha_pos - A1 * D_alpha\n",
    "\n",
    "            A2 = 2 * a * np.random.rand() - a\n",
    "            C2 = 2 * np.random.rand()\n",
    "            D_beta = np.abs(C2 * beta_pos - X_i)\n",
    "            X2 = beta_pos - A2 * D_beta\n",
    "\n",
    "            A3 = 2 * a * np.random.rand() - a\n",
    "            C3 = 2 * np.random.rand()\n",
    "            D_delta = np.abs(C3 * delta_pos - X_i)\n",
    "            X3 = delta_pos - A3 * D_delta\n",
    "\n",
    "            new_pos = (X1 + X2 + X3) / 3.0\n",
    "            new_pos = np.minimum(np.maximum(new_pos, lb), ub)\n",
    "            positions[i] = new_pos\n",
    "\n",
    "        fitness = np.array([fitness_from_params(p[0], p[1]) for p in positions])\n",
    "        idx = fitness.argsort()[::-1]\n",
    "        if float(fitness[idx[0]]) > alpha_score:\n",
    "            alpha_score = float(fitness[idx[0]])\n",
    "            alpha_pos = positions[idx[0]].copy()\n",
    "        if float(fitness[idx[1]]) > beta_score:\n",
    "            beta_score = float(fitness[idx[1]])\n",
    "            beta_pos = positions[idx[1]].copy()\n",
    "        if float(fitness[idx[2]]) > delta_score:\n",
    "            delta_score = float(fitness[idx[2]])\n",
    "            delta_pos = positions[idx[2]].copy()\n",
    "        best_history.append(alpha_score)\n",
    "\n",
    "        if verbose:\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"iter {t+1:02d}/{max_iter}  best={alpha_score:.6f}  (logC={alpha_pos[0]:+.3f}, logG={alpha_pos[1]:+.3f})  elapsed={elapsed:.1f}s\")\n",
    "\n",
    "    best_logC, best_logG = alpha_pos[0], alpha_pos[1]\n",
    "    best_C = 10 ** best_logC\n",
    "    best_gamma = 10 ** best_logG\n",
    "    best_acc = alpha_score\n",
    "    return best_C, best_gamma, best_acc, best_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    C, gamma, acc, hist = gwo(n_agents=12, max_iter=15, lb=(-2.0, -4.0), ub=(4.0, -0.5), seed=42, verbose=True)\n",
    "    print(\"\\nOptimal Parameter found\")\n",
    "    print(f\"C = {C:.6f} , gamma = {gamma:.6f}\")\n",
    "    print(f\"Best score = {acc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1da28e-d08a-406a-8788-1d73d05fe8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
